// lib/groqRouter.ts
// Groq API èª¿ç”¨å·¥å…· - è¶…å¿«é€Ÿçš„ LLM API

const GROQ_API_KEY = process.env.GROQ_API_KEY || "";
const GROQ_API_BASE = "https://api.groq.com/openai/v1/chat/completions";

type Message = {
  role: "system" | "user" | "assistant";
  content: string;
};

type GroqOptions = {
  temperature?: number;
  top_p?: number;
  max_tokens?: number;
  model?: string;
};

// æ¨è–¦çš„ Groq æ¨¡å‹ï¼ˆéƒ½æ”¯æ´ä¸­æ–‡ï¼‰
export const GROQ_MODELS = {
  // æœ€å¿«æœ€æ¨è–¦
  LLAMA_3_8B: "llama-3.1-8b-instant",
  // æ›´å¤§æ›´å¼·
  LLAMA_3_70B: "llama-3.1-70b-versatile",
  // å¹³è¡¡ç‰ˆ
  MIXTRAL: "mixtral-8x7b-32768",
  // å°è€Œå¿«
  GEMMA_7B: "gemma2-9b-it",
  // ğŸ”¥ OpenAI é–‹æºç‰ˆæœ¬
  GPT_OSS_20B: "openai/gpt-oss-20b",
  GPT_OSS_120B: "openai/gpt-oss-120b",
};

/**
 * èª¿ç”¨ Groq API
 */
export async function callGroq(
  messages: Message[],
  options: GroqOptions = {}
): Promise<{ content: string; debug?: any }> {
  
  if (!GROQ_API_KEY) {
    throw new Error("GROQ_API_KEY æœªè¨­å®š");
  }

  const model = options.model || GROQ_MODELS.LLAMA_3_8B;

  const requestBody = {
    model,
    messages,
    temperature: options.temperature || 0.7,
    top_p: options.top_p || 0.9,
    max_tokens: options.max_tokens || 512,
    stream: false,
  };

  try {
    const response = await fetch(GROQ_API_BASE, {
      method: "POST",
      headers: {
        "Authorization": `Bearer ${GROQ_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify(requestBody),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Groq API Error (${response.status}): ${errorText}`);
    }

    const data = await response.json();
    
    const content = data.choices?.[0]?.message?.content || "";

    return {
      content: content.trim(),
      debug: {
        model,
        usage: data.usage,
        finish_reason: data.choices?.[0]?.finish_reason,
      },
    };
  } catch (error: any) {
    throw new Error(`Groq API èª¿ç”¨å¤±æ•—: ${error.message}`);
  }
}

/**
 * å¸¶é‡è©¦æ©Ÿåˆ¶çš„èª¿ç”¨ï¼ˆå¤šå€‹æ¨¡å‹ fallbackï¼‰
 */
export async function callGroqWithRetry(
  messages: Message[],
  options: GroqOptions = {}
): Promise<{ content: string; debug: any }> {
  
  // æŒ‰é€Ÿåº¦å’Œç©©å®šæ€§æ’åºçš„æ¨¡å‹åˆ—è¡¨
  const models = [
    GROQ_MODELS.LLAMA_3_8B,    // æœ€å¿«
    GROQ_MODELS.GEMMA_7B,       // æ¬¡å¿«
    GROQ_MODELS.MIXTRAL,        // å‚™ç”¨
  ];

  const errors: any[] = [];

  for (let i = 0; i < models.length; i++) {
    const model = models[i];
    try {
      const result = await callGroq(messages, { ...options, model });
      return {
        content: result.content,
        debug: {
          ...result.debug,
          attemptedModels: models.slice(0, i + 1),
          success: true,
          provider: "groq",
        },
      };
    } catch (error: any) {
      errors.push({
        model,
        error: error.message,
      });
      
      // å¦‚æœä¸æ˜¯æœ€å¾Œä¸€å€‹æ¨¡å‹ï¼Œç¹¼çºŒå˜—è©¦
      if (i < models.length - 1) {
        continue;
      }
    }
  }

  // æ‰€æœ‰æ¨¡å‹éƒ½å¤±æ•—
  throw {
    message: "All Groq models failed",
    errors,
  };
}