// app/api/respond/route.ts
import { NextRequest, NextResponse } from "next/server";
import { routeToModel } from "@/lib/modelRouter";
import { saveAnalysisToFile } from "@/lib/database";
import type { BertAnalysisResult } from "@/lib/bertAnalyzer";

type ModelChoice = "llama" | "gpt" | "auto";

export async function POST(req: NextRequest) {
  try {
    const { message, analysis, history = [], model = "auto" } = await req.json();
    
    if (!message) {
      return NextResponse.json(
        { status: "error", message: "message is required" },
        { status: 400 }
      );
    }

    console.log("ğŸ¤– é–‹å§‹ç”Ÿæˆå›æ‡‰...");
    console.log("  - ç”¨æˆ¶é¸æ“‡æ¨¡å‹:", model);
    console.log("  - é¢¨éšªåˆ†æ•¸:", analysis?.risk_score || "N/A");
    console.log("  - ç·Šæ€¥ç¨‹åº¦:", analysis?.urgency_level || "N/A");

    // æ§‹å»ºå®Œæ•´çš„ BERT åˆ†æçµæœ
    const bertAnalysis: BertAnalysisResult = {
      sentiment_score: analysis?.sentiment || 0.5,
      risk_score: analysis?.risk_score || 0.3,
      outline: analysis?.outline || [],
      keywords: analysis?.keywords || [],
      categories: analysis?.categories || [],
      urgency_level: analysis?.urgency_level || "low",
      raw_text: message,
      timestamp: new Date().toISOString(),
    };

    // èª¿ç”¨æ™ºèƒ½æ¨¡å‹è·¯ç”±
    const modelResponse = await routeToModel(
      message,
      bertAnalysis,
      model as ModelChoice,
      history
    );

    console.log("âœ… å›æ‡‰ç”Ÿæˆå®Œæˆ");
    console.log("  - ä½¿ç”¨æ¨¡å‹:", modelResponse.model_used);
    console.log("  - æä¾›å•†:", modelResponse.provider);

    // å„²å­˜å®Œæ•´è¨˜éŒ„åˆ°è³‡æ–™åº«
    const recordId = `${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
    
    try {
      await saveAnalysisToFile({
        id: recordId,
        user_message: message,
        bert_analysis: bertAnalysis,
        model_response: modelResponse.content,
        model_used: modelResponse.model_used,
        created_at: new Date().toISOString(),
      });
      console.log("ğŸ’¾ è¨˜éŒ„å·²å„²å­˜:", recordId);
    } catch (saveError) {
      console.error("âš ï¸ å„²å­˜è¨˜éŒ„å¤±æ•—:", saveError);
      // ä¸å½±éŸ¿å›æ‡‰ï¼Œç¹¼çºŒåŸ·è¡Œ
    }

    return NextResponse.json({
      status: "success",
      reply: modelResponse.content,
      debug: {
        record_id: recordId,
        model_used: modelResponse.model_used,
        provider: modelResponse.provider,
        bert_analysis: bertAnalysis,
        saved_to_database: true,
      },
    });

  } catch (e: any) {
    console.error("âŒ å›æ‡‰ç”Ÿæˆå¤±æ•—:", e);
    
    // ä½¿ç”¨æœ¬åœ°å‚™æ´å›æ‡‰
    const fallbackReply = [
      "æ„Ÿè¬æ‚¨çš„è«®è©¢ã€‚é›–ç„¶ç³»çµ±æš«æ™‚ç„¡æ³•é€£æ¥åˆ° AI æ¨¡å‹ï¼Œä½†æˆ‘å¯ä»¥æä¾›ä¸€äº›åŸºæœ¬å»ºè­°ï¼š",
      "",
      "**ä¸€èˆ¬å»ºè­°**ï¼š",
      "1. å¦‚ç—‡ç‹€æŒçºŒæˆ–åŠ é‡ï¼Œè«‹ç›¡å¿«å°±é†«",
      "2. ä¿æŒå……è¶³ä¼‘æ¯å’Œæ°´åˆ†è£œå……",
      "3. è¨˜éŒ„ç—‡ç‹€çš„è®ŠåŒ–æƒ…æ³",
      "4. é¿å…è‡ªè¡Œç”¨è—¥",
      "",
      "âš ï¸ **ç·Šæ€¥æƒ…æ³**ï¼šè‹¥å‡ºç¾åŠ‡çƒˆç–¼ç—›ã€å‘¼å¸å›°é›£ã€æ„è­˜æ”¹è®Šç­‰ï¼Œè«‹ç«‹å³å°±é†«æˆ–æ’¥æ‰“119ã€‚",
      "",
      "ğŸ’¡ ä»¥ä¸Šå»ºè­°ä¸èƒ½æ›¿ä»£å°ˆæ¥­é†«ç™‚è¨ºæ–·ï¼Œå»ºè­°è«®è©¢é†«ç™‚å°ˆæ¥­äººå“¡ã€‚",
    ].join("\n");

    return NextResponse.json({
      status: "success",
      reply: fallbackReply,
      debug: {
        error: e?.message || String(e),
        fallback_used: true,
      },
    });
  }
}